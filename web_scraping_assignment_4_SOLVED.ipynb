{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEB SCRAPING – ASSIGNMENT 4\n",
    " Read all the problem statements, notes carefully and scrape the required data using any web scraping tool of\n",
    "your choice.\n",
    " You have to handle commonly occurring EXCEPTIONS by using exception handling programing. To get\n",
    "information about selenium Exceptions. You may visit following links:\n",
    "1. https://selenium-python.readthedocs.io/api.html\n",
    "2. https://www.guru99.com/exception-handling-selenium.html\n",
    "3. https://stackoverflow.com/questions/38022658/selenium-python-handling-no-such-elementexception/38023345\n",
    "1. Scrape the details of most viewed videos on YouTube from Wikipedia:\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views\n",
    "2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1st ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code.\n",
    "3. Scrape the details of selenium exception from guru99.com.\n",
    "Url = https://www.guru99.com/\n",
    "You need to find following details:\n",
    "A) Name\n",
    "B) Description\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code.\n",
    "4. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP at current price (19-20)\n",
    "D) GSDP at current price (18-19)\n",
    "E) Share(18-19)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code.\n",
    "5. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "ASSIGNMENT\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code.\n",
    "6. Scrape the details of top 100 songs on billboard.com.\n",
    "Url = https://www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code.\n",
    "7. Scrape the details of Data science recruiters from naukri.com.\n",
    "Url = https://www.naukri.com/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Designation\n",
    "C) Company\n",
    "D) Skills they hire for\n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and\n",
    "click on search. All this should be done through code\n",
    "8. Scrape the details of Highest selling novels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-greycompare/\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n",
    "9. Scrape the details most watched tv series of all time from imdb.com.\n",
    " Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes\n",
    "10. Details of Datasets from UCI machine learning repositories.\n",
    " Url = https://archive.ics.uci.edu/\n",
    " You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\public\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\public\\anaconda3\\lib\\site-packages (from selenium) (1.25.11)\n"
     ]
    }
   ],
   "source": [
    "#Let's first install the Selenium library\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's import the required libraries\n",
    "#Importing Selenium\n",
    "import selenium\n",
    "\n",
    "#Importing pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Importing Selenium webdriver \n",
    "from selenium import webdriver\n",
    "\n",
    "#Importing time for sleep()\n",
    "import time\n",
    "\n",
    "#Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "\n",
    "# importing regular expression\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scrape the details of most viewed videos on YouTube from Wikipedia:\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating driver variable and connecting to the web driver\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#firstly, get the webpage https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "driver.get('https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists to store the scraped data\n",
    "most_viewed = {}  #dictionary\n",
    "most_viewed['Rank'] = []\n",
    "most_viewed['Name'] = []\n",
    "most_viewed['Artist'] = []\n",
    "most_viewed['Upload_date'] = []\n",
    "most_viewed['Views'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the web element of rank and then scraping the text from it\n",
    "try:\n",
    "    rank = driver.find_elements_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[1]')    \n",
    "    for i in rank:\n",
    "        most_viewed['Rank'].append(i.text) \n",
    "except NoSuchElementException:\n",
    "    most_viewed['Rank'].append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the web element of Name and then scraping the text from it\n",
    "try:\n",
    "    name = driver.find_elements_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[2]')    \n",
    "    for i in name:\n",
    "        most_viewed['Name'].append(i.text) \n",
    "except NoSuchElementException:\n",
    "    most_viewed['Name'].append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the web element of Artist and then scraping the text from it\n",
    "try:\n",
    "    artist = driver.find_elements_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[3]')    \n",
    "    for i in artist:\n",
    "        most_viewed['Artist'].append(i.text) \n",
    "except NoSuchElementException:\n",
    "    most_viewed['Artist'].append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the web element of Upload_date and then scraping the text from it\n",
    "try:\n",
    "    date = driver.find_elements_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[5]')    \n",
    "    for i in date:\n",
    "        most_viewed['Upload_date'].append(i.text) \n",
    "except NoSuchElementException:\n",
    "    most_viewed['Upload_date'].append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the web element of Views and then scraping the text from it\n",
    "try:\n",
    "    view = driver.find_elements_by_xpath('//table[@class=\"wikitable sortable jquery-tablesorter\"][1]/tbody/tr/td[4]')    \n",
    "    for i in view:\n",
    "        most_viewed['Views'].append(i.text) \n",
    "except NoSuchElementException:\n",
    "    most_viewed['Views'].append(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload_date</th>\n",
       "      <th>Views</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[23]</td>\n",
       "      <td>Pinkfong Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>9.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[25]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>7.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[26]</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>5.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Shape of You\"[27]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>5.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"See You Again\"[28]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"Bath Song\"[31]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>4.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[32]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[33]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Uptown Funk\"[34]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>4.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Gangnam Style\"[35]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[37]</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>4.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Dame Tu Cosita\"[38]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>3.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Sugar\"[39]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Sorry\"[40]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Roar\"[41]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Counting Stars\"[42]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Thinking Out Loud\"[43]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Wheels on the Bus\"[44]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>3.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Dark Horse\"[45]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Faded\"[46]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Girls Like You\"[47]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Shake It Off\"[48]</td>\n",
       "      <td>Taylor Swift</td>\n",
       "      <td>August 18, 2014</td>\n",
       "      <td>3.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Lean On\"[49]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Bailando\"[50]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Let Her Go\"[51]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Axel F\"[52]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>2.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Mi Gente\"[53]</td>\n",
       "      <td>J Balvin</td>\n",
       "      <td>June 29, 2017</td>\n",
       "      <td>2.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Perfect\"[54]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[55]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>2.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Hello\"[56]</td>\n",
       "      <td>Adele</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>2.88</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                             Name  \\\n",
       "0    1.                           \"Baby Shark Dance\"[23]   \n",
       "1    2.                                  \"Despacito\"[25]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[26]   \n",
       "3    4.                               \"Shape of You\"[27]   \n",
       "4    5.                              \"See You Again\"[28]   \n",
       "5    6.                                  \"Bath Song\"[31]   \n",
       "6    7.  \"Learning Colors – Colorful Eggs on a Farm\"[32]   \n",
       "7    8.   \"Masha and the Bear – Recipe for Disaster\"[33]   \n",
       "8    9.                                \"Uptown Funk\"[34]   \n",
       "9   10.                              \"Gangnam Style\"[35]   \n",
       "10  11.                \"Phonics Song with Two Words\"[37]   \n",
       "11  12.                             \"Dame Tu Cosita\"[38]   \n",
       "12  13.                                      \"Sugar\"[39]   \n",
       "13  14.                                      \"Sorry\"[40]   \n",
       "14  15.                                       \"Roar\"[41]   \n",
       "15  16.                             \"Counting Stars\"[42]   \n",
       "16  17.                          \"Thinking Out Loud\"[43]   \n",
       "17  18.                          \"Wheels on the Bus\"[44]   \n",
       "18  19.                                 \"Dark Horse\"[45]   \n",
       "19  20.                                      \"Faded\"[46]   \n",
       "20  21.                             \"Girls Like You\"[47]   \n",
       "21  22.                               \"Shake It Off\"[48]   \n",
       "22  23.                                    \"Lean On\"[49]   \n",
       "23  24.                                   \"Bailando\"[50]   \n",
       "24  25.                                 \"Let Her Go\"[51]   \n",
       "25  26.                                     \"Axel F\"[52]   \n",
       "26  27.                                   \"Mi Gente\"[53]   \n",
       "27  28.                                    \"Perfect\"[54]   \n",
       "28  29.           \"Waka Waka (This Time for Africa)\"[55]   \n",
       "29  30.                                      \"Hello\"[56]   \n",
       "\n",
       "                            Artist        Upload_date Views  \n",
       "0   Pinkfong Kids' Songs & Stories      June 17, 2016  9.27  \n",
       "1                       Luis Fonsi   January 12, 2017  7.52  \n",
       "2                      LooLoo Kids    October 8, 2016  5.68  \n",
       "3                       Ed Sheeran   January 30, 2017  5.45  \n",
       "4                      Wiz Khalifa      April 6, 2015  5.25  \n",
       "5       Cocomelon – Nursery Rhymes        May 2, 2018  4.49  \n",
       "6                      Miroshka TV  February 27, 2018  4.46  \n",
       "7                       Get Movies   January 31, 2012  4.46  \n",
       "8                      Mark Ronson  November 19, 2014  4.28  \n",
       "9                              Psy      July 15, 2012  4.18  \n",
       "10                       ChuChu TV      March 6, 2014  4.14  \n",
       "11                       El Chombo      April 5, 2018  3.57  \n",
       "12                        Maroon 5   January 14, 2015  3.54  \n",
       "13                   Justin Bieber   October 22, 2015  3.46  \n",
       "14                      Katy Perry  September 5, 2013  3.42  \n",
       "15                     OneRepublic       May 31, 2013  3.38  \n",
       "16                      Ed Sheeran    October 7, 2014  3.31  \n",
       "17      Cocomelon – Nursery Rhymes       May 24, 2018  3.24  \n",
       "18                      Katy Perry  February 20, 2014  3.14  \n",
       "19                     Alan Walker   December 3, 2015  3.13  \n",
       "20                        Maroon 5       May 31, 2018  3.11  \n",
       "21                    Taylor Swift    August 18, 2014  3.09  \n",
       "22                     Major Lazer     March 22, 2015  3.09  \n",
       "23                Enrique Iglesias     April 11, 2014  3.09  \n",
       "24                       Passenger      July 25, 2012  3.07  \n",
       "25                      Crazy Frog      June 16, 2009  2.98  \n",
       "26                        J Balvin      June 29, 2017  2.97  \n",
       "27                      Ed Sheeran   November 9, 2017  2.94  \n",
       "28                         Shakira       June 4, 2010  2.93  \n",
       "29                           Adele   October 22, 2015  2.88  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a dataframe of scraped data\n",
    "most_viewed__df = pd.DataFrame(most_viewed)\n",
    "most_viewed__df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've sucessfully created the dataframe of most viewed youtube videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrape the details team India’s international fixtures from bcci.tv. Url = https://www.bcci.tv/. You need to find following details:\n",
    "- A) Match title (I.e. 1st ODI)\n",
    "- B) Series\n",
    "- C) Place\n",
    "- D) Date\n",
    "- E) Time\n",
    "\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.bcci.tv/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting url for international fixture and clicking on it\n",
    "Options_btn = driver.find_element_by_xpath(\"//div[@class='navigation__drop-down drop-down drop-down--reveal-on-hover']\").click()\n",
    "\n",
    "Infix_btn = driver.find_element_by_xpath(\"//a[@class='navigation__link navigation__link--in-drop-down']\").click()\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Match_title = []\n",
    "Series = []\n",
    "Place = []\n",
    "Date = []\n",
    "Time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of match title\n",
    "for i in driver.find_elements_by_xpath(\"//strong[@class='fixture__name fixture__name--with-margin']\"):\n",
    "    Match_title.append(i.text)\n",
    "    \n",
    "#Scraping data of Place\n",
    "for i in driver.find_elements_by_xpath(\"//p[@class='fixture__additional-info']//span\"):\n",
    "    Place.append(i.text)\n",
    "    \n",
    "#for scraping date and time we have go into each title    \n",
    "#Scraping url for each match\n",
    "mat_url = []\n",
    "urls = driver.find_elements_by_xpath(\"//div[@class='js-list']//a\")\n",
    "for i in urls:\n",
    "    mat_url.append(i.get_attribute(\"href\"))\n",
    "    \n",
    "#Now scraping date, time and series\n",
    "for i in mat_url:\n",
    "    driver.get(i)\n",
    "    time.sleep(8)\n",
    "    \n",
    "    \n",
    "    #Scraping data of Series\n",
    "    try:\n",
    "        series = driver.find_element_by_xpath(\"/html/body/div[2]/div/div[2]/section[3]/div/ul/li[1]/span[2]\")\n",
    "        Series.append(series.text)\n",
    "    except NoSuchElementException:\n",
    "        Series.append('-')\n",
    "        \n",
    "    #Scraping data of dates\n",
    "    try:\n",
    "        date = driver.find_element_by_xpath(\"//div[@class='mc-header-scorebox__datetime']//strong\")\n",
    "        Date.append(date.text)\n",
    "    except NoSuchElementException:\n",
    "        Date.append('-')\n",
    "        \n",
    "    #Scraping data of time\n",
    "    try:\n",
    "        times = driver.find_element_by_xpath(\"//span[@class='mc-header-scorebox__ist-time']\")\n",
    "        Time.append(times.text)\n",
    "    except NoSuchElementException:\n",
    "        Time.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "BCCI=pd.DataFrame({})\n",
    "BCCI['Match Title'] = Match_title\n",
    "BCCI['Series'] = Series\n",
    "BCCI['Place'] = Place\n",
    "BCCI['Date'] = Date\n",
    "BCCI['Time'] = Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5th Test</td>\n",
       "      <td>England v India 2021</td>\n",
       "      <td>Old Trafford, Manchester</td>\n",
       "      <td>Friday 10 September</td>\n",
       "      <td>15:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Match 16</td>\n",
       "      <td>2021 ICC Men's T20 World Cup</td>\n",
       "      <td>Dubai International Cricket Stadium, Dubai</td>\n",
       "      <td>Sunday 24 October</td>\n",
       "      <td>19:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Match 28</td>\n",
       "      <td>2021 ICC Men's T20 World Cup</td>\n",
       "      <td>Dubai International Cricket Stadium, Dubai</td>\n",
       "      <td>Sunday 31 October</td>\n",
       "      <td>19:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Match 33</td>\n",
       "      <td>2021 ICC Men's T20 World Cup</td>\n",
       "      <td>Sheikh Zayed Cricket Stadium, Abu Dhabi</td>\n",
       "      <td>Wednesday 3 November</td>\n",
       "      <td>19:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Match 37</td>\n",
       "      <td>-</td>\n",
       "      <td>Dubai International Cricket Stadium, Dubai</td>\n",
       "      <td>Friday 5 November</td>\n",
       "      <td>19:30 IST</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Match 42</td>\n",
       "      <td>-</td>\n",
       "      <td>Dubai International Cricket Stadium, Dubai</td>\n",
       "      <td>Monday 8 November</td>\n",
       "      <td>19:30 IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Match Title                        Series  \\\n",
       "0    5th Test          England v India 2021   \n",
       "1    Match 16  2021 ICC Men's T20 World Cup   \n",
       "2    Match 28  2021 ICC Men's T20 World Cup   \n",
       "3    Match 33  2021 ICC Men's T20 World Cup   \n",
       "4    Match 37                             -   \n",
       "5    Match 42                             -   \n",
       "\n",
       "                                        Place                  Date       Time  \n",
       "0                    Old Trafford, Manchester   Friday 10 September  15:30 IST  \n",
       "1  Dubai International Cricket Stadium, Dubai     Sunday 24 October  19:30 IST  \n",
       "2  Dubai International Cricket Stadium, Dubai     Sunday 31 October  19:30 IST  \n",
       "3     Sheikh Zayed Cricket Stadium, Abu Dhabi  Wednesday 3 November  19:30 IST  \n",
       "4  Dubai International Cricket Stadium, Dubai     Friday 5 November  19:30 IST  \n",
       "5  Dubai International Cricket Stadium, Dubai     Monday 8 November  19:30 IST  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Printing data frame\n",
    "BCCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the scraped data of Question 2. And i have scrapped it using selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of selenium exception from guru99.com. Url = https://www.guru99.com/ You need to find following details:\n",
    "- A) Name\n",
    "- B) Description\n",
    "\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.guru99.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[1]/div[2]/div/div[1]/div/div/div/form/table/tbody/tr/td[1]/div/table/tbody/tr/td[1]/input\"}\n  (Session info: chrome=92.0.4515.159)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-5550788f695a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Going to search bar and searching for selenium exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mser_bar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[1]/div[2]/div/div[1]/div/div/div/form/table/tbody/tr/td[1]/div/table/tbody/tr/td[1]/input\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mser_bar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"selenium exception handling\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mser_but\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[1]/div[2]/div/div[1]/div/div/div/form/table/tbody/tr/td[2]/button\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\public\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element_by_xpath\u001b[1;34m(self, xpath)\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0melement\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'//div/td[1]'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m         \"\"\"\n\u001b[1;32m--> 394\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_element\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfind_elements_by_xpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\public\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mfind_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    974\u001b[0m                 \u001b[0mby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCSS_SELECTOR\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'[name=\"%s\"]'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 976\u001b[1;33m         return self.execute(Command.FIND_ELEMENT, {\n\u001b[0m\u001b[0;32m    977\u001b[0m             \u001b[1;34m'using'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    978\u001b[0m             'value': value})['value']\n",
      "\u001b[1;32mC:\\Users\\public\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[0;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[1;32mC:\\Users\\public\\anaconda3\\lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'alert'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_value_or_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"xpath\",\"selector\":\"/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[1]/div[2]/div/div[1]/div/div/div/form/table/tbody/tr/td[1]/div/table/tbody/tr/td[1]/input\"}\n  (Session info: chrome=92.0.4515.159)\n"
     ]
    }
   ],
   "source": [
    "#Going to search bar and searching for selenium exception\n",
    "ser_bar = driver.find_element_by_xpath(\"/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[1]/div[2]/div/div[1]/div/div/div/form/table/tbody/tr/td[1]/div/table/tbody/tr/td[1]/input\")\n",
    "ser_bar.send_keys(\"selenium exception handling\")\n",
    "ser_but = driver.find_element_by_xpath(\"/html/body/div[1]/div/div/div/main/div/article/div/div[1]/div[1]/div[2]/div/div[1]/div/div/div/form/table/tbody/tr/td[2]/button\").click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Url of selenium exception handling page\n",
    "urls = driver.find_element_by_xpath(\"//div[@class='gsc-expansionArea']//div//a\")\n",
    "page_url = urls.get_attribute(\"href\")\n",
    "\n",
    "driver.get(page_url)\n",
    "time.sleep(5)\n",
    "\n",
    "#Scraping data of names of exceptions\n",
    "Name = []\n",
    "for i in driver.find_elements_by_xpath(\"//table[@class='table table-striped']//tr//td[1]\")[1:]:\n",
    "    Name.append(i.text)\n",
    "    \n",
    "#Scraping data of descriptions of exceptions  \n",
    "Description = []\n",
    "for i in driver.find_elements_by_xpath(\"//table[@class='table table-striped']//tr//td[2]\")[1:]:\n",
    "    Description.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Guru=pd.DataFrame({})\n",
    "Guru['Name of Exception'] = Name\n",
    "Guru['Description of Exception'] = Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "Guru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the scraped data of Question 3. And i have scrapped it using selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of State-wise GDP of India from statisticstime.com. Url = http://statisticstimes.com/ You have to find following details:\n",
    "- A) Rank\n",
    "- B) State\n",
    "- C) GSDP at current price (19-20)\n",
    "- D) GSDP at current price (18-19)\n",
    "- E) Share(18-19)\n",
    "- F) GDP($ billion)\n",
    "\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"http://statisticstimes.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on economy section\n",
    "economy = driver.find_element_by_xpath(\"//div[@class='navbar']//div[2]//button\").click()\n",
    "\n",
    "urls = driver.find_element_by_xpath(\"//div[@class='dropdown-content']//a[3]\")\n",
    "ineco_page = urls.get_attribute(\"href\")\n",
    "#Going to indian economy page\n",
    "driver.get(ineco_page)  \n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Rank = []\n",
    "State = []\n",
    "GSDP_1 = []\n",
    "GSDP_2 = []\n",
    "Share = []\n",
    "GDP = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the url of page containing GDP of indian states\n",
    "sta_GDP = driver.find_element_by_xpath(\"//ul[@style='list-style-type:none;margin-left:20px;']/li/a\")\n",
    "GDP_url = sta_GDP.get_attribute(\"href\")\n",
    "\n",
    "driver.get(GDP_url)\n",
    "time.sleep(6)\n",
    "\n",
    "\n",
    "#Scraping data of rank\n",
    "for i in driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']//tbody//tr//td[1]\"):\n",
    "    Rank.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of state name\n",
    "for i in driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']//tbody//tr//td[2]\"):\n",
    "    State.append(i.text)\n",
    "\n",
    "    \n",
    "# Scraping data of GSDP at current price (19-20)\n",
    "for i in driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']//tbody//tr//td[3]\"):\n",
    "    GSDP_1.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of GSDP at current price (18-19)\n",
    "for i in driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']//tbody//tr//td[4]\"):\n",
    "    GSDP_2.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Share(18-19)\n",
    "for i in driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']//tbody//tr//td[5]\"):\n",
    "    Share.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of GDP($ billion)\n",
    "for i in driver.find_elements_by_xpath(\"//div[@id='table_id_wrapper']//tbody//tr//td[6]\"):\n",
    "    GDP.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "StatisticsTime=pd.DataFrame({})\n",
    "StatisticsTime['Rank'] = Rank\n",
    "StatisticsTime['State'] = State\n",
    "StatisticsTime['GSDP at current price (19-20)'] = GSDP_1\n",
    "StatisticsTime['GSDP at current price (18-19)'] = GSDP_2\n",
    "StatisticsTime['Share(18-19)'] = Share\n",
    "StatisticsTime['GDP($ billion)'] = GDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the data frame\n",
    "StatisticsTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the scraped data of Question 4. And i have scrapped it using selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of trending repositories on Github.com. Url = https://github.com/ You have to find the following details:\n",
    "- A) Repository title\n",
    "- B) Repository description\n",
    "- C) Contributors count\n",
    "- D) Language used\n",
    "\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://github.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting explore button and clicking on it\n",
    "explore = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details\").click()\n",
    "\n",
    "#Selecting trending option\n",
    "trend_url = driver.find_element_by_xpath(\"/html/body/div[1]/header/div/div[2]/nav/ul/li[4]/details/div/ul[2]/li[3]/a\")\n",
    "urls = trend_url.get_attribute(\"href\")\n",
    "driver.get(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "repo_urls = []\n",
    "rep_title = []\n",
    "Description = []\n",
    "Contributors = []\n",
    "Language = []\n",
    "lang = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching urls for each repository\n",
    "repository = driver.find_elements_by_xpath(\"//h1[@class = 'h3 lh-condensed']//a\")\n",
    "for i in repository:\n",
    "    repo_urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "    \n",
    "#Scraping data of repository title\n",
    "title = driver.find_elements_by_xpath(\"//h1[@class = 'h3 lh-condensed']\")\n",
    "for i in title:\n",
    "    rep_title.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data from every repository page\n",
    "for i in repo_urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    #Scraping data of decription\n",
    "    try:\n",
    "        desc = driver.find_element_by_xpath(\"//p[@class='f4 mt-3']\")\n",
    "        Description.append(desc.text)\n",
    "    except NoSuchElementException:\n",
    "        Description.append('-')\n",
    "        \n",
    "        \n",
    "    #Scraping data of contributors\n",
    "    try:\n",
    "        cont_tags = driver.find_element_by_xpath(\"//*[contains(text(),'    Contributors ')]\")\n",
    "        Contributors.append(cont_tags.text.replace('Contributors',''))\n",
    "    except NoSuchElementException:\n",
    "        Contributors.append('-')\n",
    "        \n",
    "        \n",
    "    #fetching Languages used\n",
    "    try:\n",
    "        for i in driver.find_elements_by_xpath(\"//span[@class='color-text-primary text-bold mr-1']\"):\n",
    "            lang.append(i.text)\n",
    "        Language.append(lang)\n",
    "    except NoSuchElementException:\n",
    "        Language.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Github=pd.DataFrame({})\n",
    "Github['Repository title'] = rep_title\n",
    "Github['Repository description'] = Description\n",
    "Github['Contributors count'] = Contributors\n",
    "Github['Language used'] = Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "Github"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the scraped data of Question 5. And i have scrapped it using selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of top 100 songs on billboard.com. Url = https://www.billboard.com/ You have to find the following details:\n",
    "- A) Song name\n",
    "- B) Artist name\n",
    "- C) Last week rank\n",
    "- D) Peak rank\n",
    "- E) Weeks on board\n",
    "\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.billboard.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clicking on charts\n",
    "charts=driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/header/div/ul/li[1]\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Name = []\n",
    "Artist = []\n",
    "Last_week_rank = []\n",
    "Peak_rank = []\n",
    "Weeks_on_board = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting url for top 100 \n",
    "urls = driver.find_element_by_xpath(\"//div[@class='charts-landing first-group']//div//a\")\n",
    "page_url = urls.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(4)\n",
    "\n",
    "\n",
    "#Now we have to extract required info from page url\n",
    "#Scraping data of song names\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='chart-element__information__song text--truncate color--primary']\"):\n",
    "    Name.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of artist names\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='chart-element__information__artist text--truncate color--secondary']\"):\n",
    "    Artist.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of last week ranks\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--last']\"):\n",
    "    Last_week_rank.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of peak rank\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--peak']\"):\n",
    "    Peak_rank.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of weeks on board\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='chart-element__meta text--center color--secondary text--week']\"):\n",
    "    Weeks_on_board.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Billboard=pd.DataFrame({})\n",
    "Billboard['Name'] = Name\n",
    "Billboard['Artist'] = Artist\n",
    "Billboard['Last week rank'] = Last_week_rank\n",
    "Billboard['Peak rank'] = Peak_rank\n",
    "Billboard['Weeks on board'] = Weeks_on_board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing the dataframe\n",
    "Billboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the scraped data of Question 6. And i have scrapped it using selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Scrape the details of Data science recruiters from naukri.com. Url = https://www.naukri.com/ You have to find the following details:\n",
    "- A) Name\n",
    "- B) Designation\n",
    "- C) Company\n",
    "- D) Skills they hire for\n",
    "- E) Location\n",
    "\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and \n",
    "click on search. All this should be done through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching urls for recruiter page\n",
    "rec = driver.find_element_by_xpath(\"//a[@title='Search Recruiters']\")\n",
    "page_url = rec.get_attribute(\"href\")\n",
    "\n",
    "driver.get(page_url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetching search button,bar xpath and clicking on it\n",
    "search = driver.find_element_by_xpath(\"//div[@class='inpWrap']//input\") \n",
    "search.send_keys(\"Data science \")           \n",
    "but = driver.find_element_by_xpath(\"//button[@class='fl qsbSrch blueBtn']\").click()     \n",
    "time.sleep(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Name = []\n",
    "Designation = []\n",
    "Company = []\n",
    "Skills = []\n",
    "Location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Names\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='fl ellipsis']\")[:50]:\n",
    "    Name.append(i.text)\n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "#Scraping data of Designation\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='ellipsis clr']\")[:50]:\n",
    "    Designation.append(i.text)\n",
    "time.sleep(5)\n",
    "\n",
    "#Scraping data of company name\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='vcard']//p[1]/a[2]\")[:50]:\n",
    "    Company.append(i.text)\n",
    "time.sleep(5)\n",
    "    \n",
    "#scraping data of Skills \n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='hireSec highlightable']\")[:50]:\n",
    "    try:\n",
    "        if i.text == \"Not Specified\": raise NoSuchElementException\n",
    "        Skills.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Skills.append('-')\n",
    "    time.sleep(5)\n",
    "\n",
    "#Scraping data of locations\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='vcard']//p[1]//span\")[:50]:\n",
    "    Location.append(i.text)\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Naukri=pd.DataFrame({})\n",
    "Naukri['Name'] = Name\n",
    "Naukri['Designation'] = Designation\n",
    "Naukri['Company'] = Company\n",
    "Naukri['Skills'] = Skills\n",
    "Naukri['Location'] = Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "Naukri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the scraped data of Question 7. And i have scrapped it using selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Scrape the details of Highest selling novels. Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey\u0002compare/ You have to find the following details:\n",
    "- A) Book name\n",
    "- B) Author name\n",
    "- C) Volumes sold\n",
    "- D) Publisher\n",
    "- E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Book_name = []\n",
    "Author = []\n",
    "Volumes_sold = []\n",
    "Publisher = []\n",
    "Genre = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scraping data of book names\n",
    "for i in driver.find_elements_by_xpath(\"//tbody//tr/td[2]\"):\n",
    "    Book_name.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of author names\n",
    "for i in driver.find_elements_by_xpath(\"//tbody/tr/td[3]\"):\n",
    "    try:\n",
    "        if i.text == '0' : raise NoSuchElementException\n",
    "        Author.append(i.text)\n",
    "    except NoSuchElementException:\n",
    "        Author.append('-')\n",
    "time.sleep(1)\n",
    "\n",
    "\n",
    "#Scraping data of volumes sold\n",
    "for i in driver.find_elements_by_xpath(\"//tbody/tr/td[4]\"):\n",
    "    Volumes_sold.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of publisher names\n",
    "for i in driver.find_elements_by_xpath(\"//tbody/tr/td[5]\"):\n",
    "    Publisher.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of genre\n",
    "for i in driver.find_elements_by_xpath(\"//tbody/tr/td[6]\"):\n",
    "    Genre.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "Novel=pd.DataFrame({})\n",
    "Novel['Book Name'] = Book_name\n",
    "Novel['Author'] = Author\n",
    "Novel['Volume sold'] = Volumes_sold\n",
    "Novel['Publisher'] = Publisher\n",
    "Novel['Genre'] = Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "Novel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the scraped data of Question 8. And i have scrapped it using selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Scrape the details most watched tv series of all time from imdb.com. Url = https://www.imdb.com/list/ls095964455/ You have to find the following details:\n",
    "- A) Name\n",
    "- B) Year span\n",
    "- C) Genre\n",
    "- D) Run time\n",
    "- E) Ratings\n",
    "- F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty lists.\n",
    "Name = []\n",
    "Year_span = []\n",
    "Genre = []\n",
    "Run_time = []\n",
    "Ratings = []\n",
    "Votes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping data of Names\n",
    "for i in driver.find_elements_by_xpath(\"//h3[@class='lister-item-header']/a\"):\n",
    "    Name.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Year span\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='lister-item-year text-muted unbold']\"):\n",
    "    Year_span.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of genre\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='genre']\"):\n",
    "    Genre.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Run time\n",
    "for i in driver.find_elements_by_xpath(\"//span[@class='runtime']\"):\n",
    "    Run_time.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of Ratings\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='ipl-rating-star small']//span[2]\"):\n",
    "    Ratings.append(i.text)\n",
    "\n",
    "    \n",
    "#Scraping data of votes\n",
    "for i in driver.find_elements_by_xpath(\"//div[@class='lister-item-content']//p[4]/span[2]\"):\n",
    "    Votes.append(i.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "IMDB=pd.DataFrame({})\n",
    "IMDB['Name'] = Name\n",
    "IMDB['Year Span'] = Year_span\n",
    "IMDB['Genre'] = Genre\n",
    "IMDB['Run Time'] = Run_time\n",
    "IMDB['Ratings'] = Ratings\n",
    "IMDB['Votes'] = Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing data frame\n",
    "IMDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the scraped data of Question 9. And i have scrapped it using selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Details of Datasets from UCI machine learning repositories. Url = https://archive.ics.uci.edu/ You have to find the following details:\n",
    "- A) Dataset name\n",
    "- B) Data type\n",
    "- C) Task\n",
    "- D) Attribute type\n",
    "- E) No of instances\n",
    "- F) No of attribute\n",
    "- G) Year\n",
    "\n",
    "Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activating the chrome browser\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the specified url\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "driver.get(url)\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding view all dataset button\n",
    "view_dataset = driver.find_element_by_xpath(\"//tbody[1]//tr/td[2]/span[2]/a\")    \n",
    "page_url = view_dataset.get_attribute(\"href\")\n",
    "driver.get(page_url)\n",
    "time.sleep(5)\n",
    "\n",
    "#Getting page urls containing list of all datasets\n",
    "all_lst = driver.find_element_by_xpath(\"/html/body/table[2]/tbody/tr/td[2]/table[1]/tbody/tr/td[2]/p/a\")  \n",
    "lst_url = all_lst.get_attribute(\"href\")           \n",
    "driver.get(lst_url)\n",
    "time.sleep(5)\n",
    "\n",
    "#fetching url for each dataset\n",
    "data_url = driver.find_elements_by_xpath(\"//p[@class='normal']//b/a\")    \n",
    "\n",
    "urls = []     \n",
    "for i in data_url:\n",
    "    urls.append(i.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty list\n",
    "Data_name = []\n",
    "Data_type = []\n",
    "Task = []\n",
    "Attr_type = []\n",
    "Instances = []\n",
    "n_attributes = []\n",
    "Year = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping requiredinfo from urls\n",
    "for i in urls:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    \n",
    "    #Scraping Dataset name\n",
    "    try: \n",
    "        ds_name = driver.find_element_by_xpath(\"//span[@class='heading']\")\n",
    "        Data_name.append(ds_name.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_name.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping data type\n",
    "    try:\n",
    "        dtype = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr/td[2]\")\n",
    "        if dtype.text == \"N/A\": raise NoSuchElementException\n",
    "        Data_type.append(dtype.text)\n",
    "    except NoSuchElementException:\n",
    "        Data_type.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping Attribute type\n",
    "    try:\n",
    "        atype = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr[2]/td[2]\")\n",
    "        if atype.text == \"N/A\": raise NoSuchElementException\n",
    "        Attr_type.append(atype.text)\n",
    "    except NoSuchElementException:\n",
    "        Attr_type.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #scraping Task\n",
    "    try:\n",
    "        task = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr[3]/td[2]\")\n",
    "        if task.text == \"N/A\": raise NoSuchElementException\n",
    "        Task.append(task.text)\n",
    "    except NoSuchElementException:\n",
    "        Task.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping No of instances\n",
    "    try:\n",
    "        inst = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr/td[4]\")\n",
    "        if inst.text == \"N/A\": raise NoSuchElementException\n",
    "        Instances.append(inst.text)\n",
    "    except NoSuchElementException:\n",
    "        Instances.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #Scraping  No of attribute\n",
    "    try:\n",
    "        attr = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr[2]/td[4]\")\n",
    "        if attr.text == \"N/A\": raise NoSuchElementException\n",
    "        n_attributes.append(attr.text)\n",
    "    except NoSuchElementException:\n",
    "        n_attributes.append('-')\n",
    "    time.sleep(1)\n",
    "    \n",
    "    \n",
    "    #scraping year\n",
    "    try:\n",
    "        year = driver.find_element_by_xpath(\"//table[@border='1']//tbody/tr[2]/td[6]\")\n",
    "        if year.text == \"N/A\": raise NoSuchElementException\n",
    "        Year.append(year.text[:4])\n",
    "    except NoSuchElementException:\n",
    "        Year.append('-')\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA FRAMEING\n",
    "UCI=pd.DataFrame({})\n",
    "UCI['Data Name'] = Data_name\n",
    "UCI['Data Type'] = Data_type\n",
    "UCI['Task'] = Task\n",
    "UCI['Attribute type'] = Attr_type\n",
    "UCI['No of instance'] = Instances\n",
    "UCI['No of attributes'] = n_attributes\n",
    "UCI['Year'] = Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing dataframe\n",
    "UCI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the scraped data of Question 10. And i have scrapped it using selenium."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
